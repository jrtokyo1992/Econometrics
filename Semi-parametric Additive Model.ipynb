{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Method 1: Robinson Difference Estimator\n",
    "We first introducce a method, which is useful when dealing with additive model with only two additive components.Consider the following the model:\n",
    "$$\n",
    "y_i = X'_i\\beta + g(z_i)+\\epsilon_i \\tag{1}\n",
    "$$\n",
    "In which $g(z_i)$ is an unknown form. It seems not easy to estimate the $\\beta$ since we even do not know the $g$. But Robinson(1989) puts up a Robinson Difference Estimator.Suppose that the $E(\\epsilon|x,z)=0$. We notice that\n",
    "$$\n",
    "E(y_i|z_i) = E(X'_i |z_i)\\beta + g(z_i)+ E(\\epsilon |z_i)= E(X'_i |z_i)\\beta + g(z_i)+ E_{X_i}[ E(  \\epsilon |z_i ,X_i)  ] = E(X'_i |z_i)\\beta + g(z_i) \\tag{2}\n",
    "$$\n",
    "This implies that we get (1)-(2) as\n",
    "$$\n",
    "y_i-E(y_i|z_i)= [X'_i-E(X'_i |z_i) ]\\beta+\\epsilon_i \\tag{3}\n",
    "$$\n",
    "Then things become easy. We can first estimate $\\hat{E}(y_i|z_i)$ and $\\hat{E}(X'_i |z_i)$ (via non-parametric methods) and then estimate the $\\beta$ from:\n",
    "$$\n",
    "y_i-\\hat{E}(y_i|z_i)= [X'_i-\\hat{E}(X'_i |z_i) ]\\beta+\\epsilon_i \\tag{4}\n",
    "$$\n",
    "Finally we can get the non-parametric for $g(z_i)$ \n",
    "$$\n",
    "\\hat{g}(z_i)= \\hat{E}(y_i|z_i)-\\hat{E}(x_i|z_i)'\\hat{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating method 2: Back Fitting\n",
    "## Linear Case\n",
    "We start by first considering the following the linear regression model:\n",
    "$$\n",
    "E[Y|X]=\\sum_{j}^{p}{\\beta_j}X_j\n",
    "$$\n",
    "Then we can know that \n",
    "$$\n",
    "E[Y|X_k=x]= \\beta_k x + E\\left(\\sum_{j\\ne k}{\\beta_j X_j} | X_k= x\\right)\n",
    "$$\n",
    "which implies that \n",
    "$$\\beta_k x =E\\left(Y-\\sum_{j\\ne k}{\\beta_j X_j} | X_k= x\\right)$$\n",
    "Denote \n",
    "$$Y-\\sum_{j\\ne k}{\\beta_j X_j}\\equiv Y^{(k)} \\tag{5}$$\n",
    "If we know the data of $Y^{(k)}$, we can easily fit $\\beta_k$, since this is just single variable linear regression. The $Y^{(k)}$, in turn, relies on $\\beta_0,\\beta_2,,..\\beta_{k-1},\\beta_{k+1},...$. This motivates us to apply a recursive method to estimate all the $\\beta$. \n",
    "\n",
    "- suppose $X_0$ is constant. we first initialize $\\beta$\n",
    "- do:\n",
    "    - 1.for each $k=1,2,...p$:\n",
    "        - calculate the data $Y_k$ according to (5)\n",
    "        - regress $Y_k$ on data $X_k$ and get the estimate $\\beta^{new}_{k}$\n",
    "    - 2.when $dist(\\beta_{new},\\beta)<\\epsilon$, then stop!\n",
    "    - else: $\\beta^{new}$ $\\rightarrow$ $\\beta$ and repeat from 1\n",
    "- Finally, $\\beta_0= \\overline{y}- \\sum_{k=1}^{p}{\\beta_k \\overline{X_k}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackFittingLinear(y_data,x_data, criterion = 1e-5, max_iter = 100):\n",
    "    y = y_data \n",
    "    x = x_data\n",
    "    iter = 1\n",
    "    n = len(y)\n",
    "    nk = np.shape(x)[1]\n",
    "    beta = [0 for i in range(nk)]\n",
    "    beta_new =  beta.copy()\n",
    "    while (1):\n",
    "        # for each k from 1 to nk, calculate the updated beta_k \n",
    "        for k in range(1,nk):\n",
    "            # calculate the data for y_k\n",
    "            y_k = [y[i]-sum(list(map(lambda a,b: a*b, beta,x[i])))+beta[k]*x[i][k] for i in range(n)]\n",
    "            # now we give a new estimation for the beta_k\n",
    "            # first, extract out the data of x_k\n",
    "            x_k = [x[i][k] for i in range(n)]\n",
    "            # Then, calculate the beta_new[k]\n",
    "            beta_new[k]= sum(list (map(lambda a,b: a*b, x_k, y_k)))/sum ([x**2 for x in x_k])\n",
    "        \n",
    "        error = sum(list (map(lambda a,b: (a-b)**2, beta,beta_new )))\n",
    "        if (error <criterion or iter >max_iter):\n",
    "            break\n",
    "        beta = beta_new.copy()\n",
    "        iter = iter +1\n",
    "    \n",
    "    # finally calculate the beta[0]\n",
    "    beta[0]= sum(y)/n- sum([sum([x[i][k] for i in range(n)])*beta[k]/n for k in range(1, nk)])\n",
    "    return beta, error\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimation is [13.731748515237767, 1.0822177414989085, -0.49657320070944194]\n",
      "the error is 7.543939753052188e-06\n"
     ]
    }
   ],
   "source": [
    "# A test\n",
    "# input x and y data.\n",
    "x= np.array([1,0,3,4,9,23,3,7,3,4,23,4,3,42,20]).reshape(5,3)\n",
    "y =[34,13,2,78,3]\n",
    "beta, error = BackFittingLinear(y,x)\n",
    "print('the estimation is', beta)\n",
    "print('the error is', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Case\n",
    "One cute thing about backfitting is that it doesnâ€™t actually rely on the model\n",
    "being linear.Now consider:\n",
    "$$\n",
    "E[Y|X]=\\alpha+\\sum_{j}^{p}f_j(X_j) \\tag{6}\n",
    "$$\n",
    "And we do not know the functional form of $f$. \n",
    "Denote \n",
    "$$Y-\\alpha-\\sum_{j\\ne k}{f_j( X_j)}\\equiv Y^{(k)} $$\n",
    "Then theoretically we can do the back fitting method as in the linear case. We also have the problem of identifiability: suppose $f_1,..f_p$ and $\\alpha$ satisfy (6), then $f_1-c,..f_p-c$ and $\\alpha+pc$ also satisfy (6). This implies that, in order to make them identifiable, we need to add a constraint on $f$. Such constraint can be :\n",
    "$$\\sum_{i}^{n}{f_j(X_{ki})}=0, \\forall k=1,,,p$$\n",
    "which implies that $$\\alpha= \\overline{y}$$\n",
    " <p>We apply the following algorithm: </p>\n",
    "\n",
    "-  first initialize $f_1,...f_p$\n",
    "- do:\n",
    "    - 1.for each $k=1,2,...p$:\n",
    "        - calculate the data $Y_k$ according to (6)\n",
    "        - find a $g_k()$ that can fit data $(X_k,Y_k)$.Can apply nonparametric methods or interporlation.\n",
    "        - $f^{new}_k(.)=g_k(.)-\\frac{1}{n}\\sum{g_k(X_{ki})}$\n",
    "    - 2.when $dist(f^{new},f)<\\epsilon$, then stop!\n",
    "    - else: $f^{new}$ $\\rightarrow$ $f$ and repeat from 1\n",
    "- Finally, $\\alpha = \\overline{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "https://www.stat.cmu.edu/~ryantibs/advmethods/notes/addmodels.pdf\n",
    "https://www4.stat.ncsu.edu/~lu/ST7901/lecture%20notes/2019Lect19_GAM.pdf\n",
    "https://www.stat.cmu.edu/~cshalizi/350/lectures/21/lecture-21.pdf\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
