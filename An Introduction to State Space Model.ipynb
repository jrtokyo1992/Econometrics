{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Idea\n",
    "Denote $s_t$ as a state variable, which cannot be observed, and $y_t$ as the measurement of $s_t$, which is observable. denote $Y_t$ as the sequence $y_1,y_2,,,,y_t$. A state space model is given by:\n",
    "\n",
    "- The probability of $s_t$ given the $s_{t-1}$, and $Y_{t-1}$\n",
    "$$M(s_t | s_{t-1},Y_{t-1},\\theta) \\tag{1}$$\n",
    "\n",
    "- The probability of $y_t$ given the $s_t$ and $Y_{t-1}$.\n",
    "$$g(y_t | s_t,Y_{t-1},\\gamma ) \\tag{2}$$\n",
    "\n",
    "In which $\\theta$ and $\\gamma$ are parameters. We we want to estimate $\\theta$ and $\\gamma$.But how?  The most used method is still MLE. Since we can only observe $y_1,y_2,....y_T$, the likelihood function for the total sample is\n",
    "$$f(y_1,y_2,...y_T)=f_y(y_1)f_y(y_2|y_1)f_y(y_3|y_2,y_1)...=f_y(y_1)\\prod_{2}^{T}{f_y(y_t | Y_{t-1})}$$\n",
    "Here $f_y$ is the probability distribution of $y$. Therefore we need to find out the expression for $f_y(y_t | Y_{t-1})$. In fact this is easy to poccess.According to definition, we have\n",
    "$$f_y(y_t |Y_{t-1})= \\int{g(y_t |s_t,Y_{t-1})f_s(s_t|Y_{t-1})}ds_t \\tag{3} $$\n",
    "Here $f_s$ denotes the probabilty distribution function of $s$.$g(y_t |s_t,Y_{t-1})$is given, but $f_s(s_t|Y_{t-1})$ is still unknown. But we have \n",
    "$$f_s(s_t|Y_{t-1})=\\int{M(s_t|s_{t-1},Y_{t-1})f_s(s_{t-1}|Y_{t-1})}ds_{t-1} \\tag{4}$$\n",
    "Therefore, from (3) and (4) we know that we only need to know $f_s(s_{t-1}|Y_{t-1})$ if we want to know $f_y(y_t |Y_{t-1})$. (There is nothing new here in the derivation, only a combination of conditional probability).\n",
    "<p> Our next step is to get the expression of $f_s(s_{t-1}| Y_{t-1})$, but we can first see $f(s_{t}| Y_{t})$. Logically, $Y_t$ is determined by $s_t$, therefore what we are looking at is like a posterior 'belief' on $s_t$ after we observe $Y_t$. Therefore we use Bayesian formula and get:\n",
    "    $$f_s(s_{t}| Y_{t})=\\frac{f(s_t,Y_t)}{f(Y_t)}=\\frac{f(s_t,y_t,Y_{t-1})}{f(y_t,Y_{t-1})}=\\frac{g(y_t|s_t,Y_{t-1})f(s_t,Y_{t-1})}{f_y(y_t|Y_{t-1})f(Y_{t-1})}=\\frac{g(y_t|s_t,Y_{t-1})f_s(s_t|Y_{t-1})}{f_y(y_t|Y_{t-1})} \\tag{5}$$\n",
    "    Combining (3)(4)(5) :if we know $f_s(s_{t-1}|Y_{t-1})$, we can get $f_s(s_{t}| Y_{t})$. Therefore, starting from a given $f_s(s_0|Y_0)$ we can use derive the whole likelihood function recursively according to (3)(4)(5).</p>\n",
    "\n",
    "- use $f_s(s_{t-1}| Y_{t-1})$ to get$f_s(s_t|Y_{t-1})$ and $f_y(y_t |Y_{t-1})$  by (3) and (4)\n",
    "- use (5) to get $f_s(s_{t}| Y_{t})$\n",
    "\n",
    "Through the whole process we get the whole series of expression for $f_y(y_t |Y_{t-1})$.If all the variables are discrete, the calculation may be easy; when the variables are continuous, however, (3) and (4) requires integral which is often difficult, except for the case where the variables are generated from normal distribution. Above process is call Kalma Filter when we assume normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalma Filter & Smoother\n",
    "We assume that the state space model now looks like\n",
    "$$s_t = Ts_{t-1}+R\\eta_t,\\tag{6}$$\n",
    "$$y_t= Zs_t +S\\xi_t \\tag{7}$$\n",
    "in which $\\eta_t$ and $\\xi_t$ are independent and $\\eta_t \\~N(0,Q)$ and $\\xi_t \\~N(0,H)$. We repeat the procedure in the previous section to get the estimation $T,R,Z,S,Q,H$. The procedure is called kalma filter.\n",
    "## Kalma Filter\n",
    ". Following the steps in the basic idea,we want to know the distribution of the following three items:\n",
    "$$s_t|Y_{t-1}, y_t|Y_{t-1}, \\rightarrow s_t | Y_t$$.\n",
    "A good point on the normal distribution is that, if we assume $s_0$ follows a normal distribution, then all the $y_t, Y_t, s_t$  also follow normal distribution, and $s_t|Y_{t-1}, y_t|Y_{t-1},  s_t | Y_t$ also follows normal distributions. Denote\n",
    "$$s_t|Y_{t-1}\\~N(\\alpha_{t|t-1},P_{t|t-1}), y_t|Y_{t-1} \\~N(\\mu_{t|t-1},F_t), s_t | Y_t \\~N(\\alpha_{t|t},P_{t|t})$$\n",
    "The last one implies that $s_{t-1} | Y_{t-1} \\~N(\\alpha_{t-1|t-1},P_{t-1|t-1})$.Apparently, given the information on $Y_{t-1}$, we have according to (6) that \n",
    "$$\\alpha_{t|t-1}=T \\alpha_{t-1|t-1},\\tag{8}$$\n",
    "$$P_{t|t-1}=T P_{t-1|t-1}T'+RQR',\\tag{9}$$\n",
    "We have according to (7) that \n",
    "$$\\mu_{t|t-1}=Z \\alpha_{t|t-1}=ZT \\alpha_{t-1|t-1},\\tag{10}$$\n",
    "$$F_t = Z P_{t|t-1}Z' + SHS',\\tag{11}$$\n",
    "Therefore, starting from the distribution of $s_{t-1}|Y_{t-1}$, which is characterized by $\\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$, we can get distribution of $s_{t-1}|Y_{t-1}$, which is characterized by $\\alpha_{t|t-1}$ and $P_{t|t-1}$, and the distribution of $y_{t-1}|Y_{t-1}$, which is characterized by $\\mu_{t|t-1}$ and $F_t$.\n",
    "But again here is a question: How should we get $\\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$?\n",
    "<p> Again, we approach this problem by checking the distribution of $s_{t}|Y_{t}$, which provides information on $\\alpha_{t|t}$ and $P_{t|t}$. which is equivalent to checking the distribution of $s_{t}|y_{t}$ given the $Y_{t-1}$. Since both $s_t$ and $y_t$ follows normal distribution, statisitcs textbook tells us that the joint distribution of $s_t$ and $y_t$ is also normal, and we can calculate the distribution of $s_{t}|y_{t}$ given the their joint distribution. </p>\n",
    "<p> Suppose we already know the mean and variance of the $s_t$ and $y_t$ respectively, given $Y_{t-1}$. We can immediately write down the joint distribution of $s_t$ and $y_t$ if we know $cov(s_t,y_t)$, and this is easy. Notice that (given $Y_{t-1}$)\n",
    "    $$cov(s_t,y_t)=cov(s_t, Zs_t +S\\xi_t \\tag{7})=var(s_t)Z'=P_{t|t-1}Z'$$\n",
    "    Textbook immediately tells us that us \n",
    "    $$s_t |y_t \\~ N\\left(\\alpha_{t|t-1}+P_{t|t-1}Z'F^{-1}_t(y_t-\\mu_{t|t-1})),P_{t|t-1}+P_{t|t-1}Z'F^{-1}_tZP_{t|t-1}\\right)$$\n",
    "    which implies that\n",
    "    $$\\alpha_{t|t}=\\alpha_{t|t-1}+P_{t|t-1}Z'F^{-1}_t(y_t-\\mu_{t|t-1})),P_{t|t}=P_{t|t-1}+P_{t|t-1}Z'F^{-1}_tZP_{t|t-1} \\tag{12}$$\n",
    "    Now the procedure is very clear: </p>\n",
    "    \n",
    "- given the distribution of $s_{t-1}| y_{t-1}$, i.e., $\\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$, we get the $\\alpha_{t|t-1}$ by (8), get the $P_{t|t-1}$ by (9), get the $\\mu_{t|t-1}$ by (10), and get the $F_t$ by (11).\n",
    "- get the $\\alpha_{t|t}$ and $P_{t|t}$ by (12)\n",
    "\n",
    "## Karma Smoother \n",
    "Sometimes, we want to estimate the $s_t$ given the whole observed series $Y_T$. i.e., we want to see \n",
    "$$E(s_t| Y_T) \\equiv \\beta_{t|T}$$\n",
    "To address this, first notice that $s_t$ is directly linked to $s_{t+1}$ and $Y_t$. Intuitively, if the information of $s_{t+1}$ is given, we actually do not need the information on $y_{t+1}$ and so on ( <b> the reason is that, given $s_{t+1}$, $y_{t+1}$ and on are independent of $s_t$: given $s_{t+1}$, the variance of $y_{t+1}$ is caused by $\\xi_{t+1}$, while the variance of $s_t$ is given by $\\eta_t$. $\\eta_t$ and $\\xi_{t+1}$ are independent. </b>). Therefore our key task is to get \n",
    "$$E(s_t | Y_t, s_{t+1})$$\n",
    "This is easy if we know the joing distribution of $s_t$ and $s_{t+1}$ given the $Y_t$. Recall that \n",
    "$$s_t | Y_t \\~N(\\alpha_{t|t},P_{t|t}),s_{t+1} | Y_t \\~N(\\alpha_{t+1|t},P_{t+1|t})$$\n",
    "and we have \n",
    "$$cov(s_t,s_{t+1})=Tvar(s_t)=TP_{t|t}$$\n",
    "Therefore, again, we have \n",
    "$$E(s_t |s_{t+1},Y_t)=\\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(s_{t+1}-\\alpha_{t+1|t})$$\n",
    "since, given $s_{t+1}$ and $Y_t$, the distribution of $s_t$ is independent of $y_{t'}$ for $t'>t$, we have\n",
    "$$E(s_t |s_{t+1},Y_T)=E(s_t |s_{t+1},Y_t)=\\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(s_{t+1}-\\alpha_{t+1|t})$$\n",
    "But what we need is $E(s_t |Y_t)$, there we take expectation of $E(s_t |s_{t+1},Y_T)$ w.r.t $s_{t+1}$, and have\n",
    "$$\\beta_{t|T}\\equiv E(s_t |Y_T)=\\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(E(s_{t+1}|Y_T)-\\alpha_{t+1|t})=\\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}( \\beta_{t+1|T}-\\alpha_{t+1|t})$$\n",
    "Therefore we can calculate the $ \\beta_{t|T}$ recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the State Space Model\n",
    "The state space model is useful in estimating the $MA$ and $ARIMA$ model, which are pretty hard to write down the MLE if without a state space model specification. See the following links for further information:\n",
    "https://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/MIT14_384F13_lec21.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
