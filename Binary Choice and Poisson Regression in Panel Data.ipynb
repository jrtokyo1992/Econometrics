{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Probit/Logit Model \n",
    "We consider the following model \n",
    "$$y^{*}_{it}= x_{it}\\beta+ u_i +\\epsilon_{it}$$\n",
    "In which $u_i$ is time-invariant unobservable term and \n",
    "$$y_{it}= 1 (0)$ if $y^{*}_{it}>0 (<0)$$\n",
    "of course we still have \n",
    "$$pr(y_{it}=1 |x_{it},u_i)= pr(\\epsilon_{it}>-x_{it}\\beta- u_i)$$\n",
    "if $\\epsilon_{it}$ follows normal distribution, then \n",
    "$$pr(y_{it}=1 |x_{it},u_i)=\\Phi(-x_{it}\\beta- u_i)$$\n",
    "if $\\epsilon_{it}$ follows logit distribution, then \n",
    "$$pr(y_{it}=1 |x_{it},u_i)=\\frac{exp(x'_{it}\\beta +u_i)}{1+exp(x'_{it}\\beta+u_i)}$$\n",
    "Under either condition, we can write down the likelihood of a person $i$:\n",
    "$$f(y_{i1},,,y_{iT},u_i | x_{i1},,,x_{iT})\\equiv\\prod_{t=1}^{T}{ {\\left [ pr(y_{it}=1 |x_{it},u_i) \\right]}^{y_{it}}{\\left[pr(y_{it}=0 |x_{it},u_i)\\right]}^{1-y_{it}} }$$\n",
    "$$=\\prod_{t=1}^{T}{ {\\left [ \\Lambda(x'_{it}\\beta+u_i) \\right]}^{y_{it}}{\\left[1-  \\Lambda(x'_{it}\\beta+u_i)\\right]}^{1-y_{it}} }$$\n",
    "But notice! Here we have $u_i$ in the likelihood function. We want it to disappear. How to do this?\n",
    "notice that\n",
    "$$f(y_{i1},,,y_{iT},u_i | x_{i1},,,x_{iT})=f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i |x_{i1},,,x_{iT})$$\n",
    "in which we use $g$ to denote the distribution of $u_i$.Therefore we have\n",
    "$$f_y(y_{i1},,,y_{iT}| x_{i1},,,x_{iT})=\\int{f(y_{i1},,,y_{iT},u_i | x_{i1},,,x_{iT})}du_i$$\n",
    "$$=\\int{f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i |x_{i1},,,x_{iT})}du_i \\tag{1}$$\n",
    "## $u_i$ Independent of $x_{i1},,x_{iT}$\n",
    "This is random effect model .Under this assumption, it is quite easy to poccess. The likelihood function for person $i$ is now\n",
    "$$\\int{f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i |x_{i1},,,x_{iT})}du_i$$\n",
    "$$=\\int{f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i)}du_i$$\n",
    "$$=\\int{ \\prod_{t=1}^{T}{ {\\left [ \\Lambda(x'_{it}\\beta+u_i) \\right]}^{y_{it}}{\\left[1-  \\Lambda(x'_{it}\\beta+u_i)\\right]}^{1-y_{it}} }  g(u_i)}du_i$$\n",
    "We can calculate this integral numerically if we know the distribution of $g(u_i)$.\n",
    "\n",
    "## $u_i$ Not independent of  $x_{i1},,x_{iT}$\n",
    "This is fixed effect model. Under this assumption, it is often very hard to calculate $g(u_i |x_{i1},,,x_{iT})$! Then, following the idea in the linear panel model, we may want to get rid of $u_i$, i.e., let the $u_i$ disappear in the likelihood function!\n",
    "<p> To do this, let's see the (1) again. The distribution of $y_{i1},,,y_{iT}$ depends on $u_i$. Recall the definition of'sufficient statistics': conditioning on the sufficient statistics, the distribution of sample has nothing to do with population parameter. <b>Can we find a 'suffificent staisitcs' for $y_{i1},,,y_{iT}$ such that, conditioned on the this statitics, the joint distribution of $y_{i1},,,y_{iT}$ does not depend on $u_i$?</b></p>\n",
    "<p> It turns out that <b> if the $\\epsilon_{it}$ follows logit distribution, and by conditioning on $\\sum_{t=1}^{T}{y_{it}}=s$, the conditional likelihood function for $y_{i1},,,y_{iT}$ would no longer contain the $\\mu_i$</b></p>\n",
    "<p> To make this idea concrete, we assume there is only two periods, 1, and 2. Thereful we have </p>\n",
    "- If $\\sum_{t=1}^{T}{y_{it}}=0$, Then it is certain that $y_{i1}=y_{i2}=0$. Therefore we have \n",
    "$$pr(y_{i1}=0,y_{i2}=0|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=0)=1$$\n",
    "- If $\\sum_{t=1}^{T}{y_{it}}=2$, Then it is certain that $y_{i1}=y_{i2}=1$. Therefore we have \n",
    "$$pr(y_{i1}=1,y_{i2}=1|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=2)=1$$\n",
    "- If $\\sum_{t=1}^{T}{y_{it}}=1$, Then there are two possible cases: $y_{i1}=1,y_{i2}=0$,or $y_{i1}=0,y_{i2}=1$\n",
    "Therefore we have\n",
    "$$pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=1)=\\frac{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})}{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})+pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2})}$$\n",
    "$$= \\frac{\\frac{exp(x'_{i1}\\beta+u_i)}{1+exp(x'_{i1}\\beta+u_i)}\\frac{1}{1+exp(x'_{i2}\\beta+u_i)}}{\\frac{exp(x'_{i1}\\beta+u_i)}{1+exp(x'_{i1}\\beta+u_i)}\\frac{1}{1+exp(x'_{i2}\\beta+u_i)} + \\frac{exp(x'_{i2}\\beta+u_i)}{1+exp(x'_{i2}\\beta+u_i)}\\frac{1}{1+exp(x'_{i1}\\beta+u_i)}}=\\frac{exp(x'_{i1}\\beta+u_i)}{exp(x'_{i1}\\beta+u_i)+exp(x'_{i2}\\beta+u_i)}$$\n",
    "$$=\\frac{exp(x'_{i1}\\beta)}{exp(x'_{i1}\\beta)+exp(x'_{i2}\\beta)}$$\n",
    "similarly we have\n",
    "$$pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=1)=\\frac{exp(x'_{i2}\\beta)}{exp(x'_{i1}\\beta)+exp(x'_{i2}\\beta)}$$\n",
    "We can see that, due to the logit distribution, the $u_i$ directly disappears. In summary, the above analysis implies that, for each individual, conditioning on his $\\sum_{t=1}^{T}{y_{it}}$, we can easily find his 'conditional likelihood function' $$f(y_{i1},,,y_{iT}| u_i, \\sum_{t=1}^{T}{y_{it}} )=f(y_{i1},,,y_{iT}|  \\sum_{t=1}^{T}{y_{it}} )$$\n",
    "which is free of $u_i$. It is easy to see:\n",
    "\n",
    "- Advantage: Since we only focus on the conditional likelihood function, then $u_i$ is thrown away, and we do not need to do the integral. very convenient!\n",
    "\n",
    "- Disadvange: We lose the information for those individuals with $\\sum_{t=1}^{T}{y_{it}}=T$ or 0.\n",
    "\n",
    "In general, check the likelihood function condition on some statisitics may be a good way to kill the undesirable parameters (in the cost of losing informaion). Similar technics are used in the Cox model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Regression in Panel Data\n",
    "We next consider a model in which $y_{it}$ is non-negative integer. Specifically,\n",
    "$$pr(y_{it}=y|u_i,x_{it})=\\frac{e^{-\\lambda_i(x_{it}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{it}\\beta + \\mu_i)\\right]}^{y}}{y!}$$\n",
    "in which $$\\lambda_i(x_{it}\\beta + \\mu_i)= e^{x_{it}\\beta + \\mu_i}$$\n",
    "Therefore we can write down the likelihood function for $i$:\n",
    "$$f(y_{i1},,,y_{iT}|u_i, x_{i1},..,x_{iT})= \\prod_{t=1}^{T}{\\frac{e^{-\\lambda_i(x_{it}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{it}\\beta + \\mu_i)\\right]}^{y_{it}}}{y_{it}!}} $$\n",
    "Of course, since $u_i$ is unobservable, we still want to do take integral over it, as is shown in (1)\n",
    "## $u_i$ Independent of $x_{i1},,x_{iT}$\n",
    "This is random effect model .Under this assumption, it is quite easy to poccess. The likelihood function for person $i$ is now\n",
    "$$\\int{f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i |x_{i1},,,x_{iT})}du_i$$\n",
    "$$=\\int{f(y_{i1},,,y_{iT}|u_i , x_{i1},,,x_{iT})g(u_i)}du_i$$\n",
    "$$=\\int{ \\prod_{t=1}^{T}{\\frac{e^{-\\lambda_i(x_{it}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{it}\\beta + \\mu_i)\\right]}^{y_{it}}}{y_{it}!}}  g(u_i)}du_i$$\n",
    "We can calculate this integral numerically if we know the distribution of $g(u_i)$.\n",
    "## $u_i$ Not independent of  $x_{i1},,x_{iT}$\n",
    "In this situation we have fixed effect model. As is in the panel logit model, we want to write down the 'conditional likelihood function'. We still take $T=2$ as example. Suppose individual $i$ has $y_{i1}+y_{i2}=1$, and we want to see $f(y_{i1},y_{i2}| u_i, x_{i1},x_{i2},y_{i1}+y_{i2}=1 )$.There are two possible cases: $y_{i1}=1,y_{i2}=0$,or $y_{i1}=0,y_{i2}=1$.\n",
    "$$pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=1)=\\frac{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})}{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})+pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2})}$$\n",
    "Since\n",
    "$$pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})=\\frac{e^{-\\lambda_i(x_{i1}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{i1}\\beta + \\mu_i)\\right]}^{1}}{1!}\\frac{e^{-\\lambda_i(x_{i2}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{i2}\\beta + \\mu_i)\\right]}^{0}}{0!}$$\n",
    "$$pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2})=\\frac{e^{-\\lambda_i(x_{i1}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{i1}\\beta + \\mu_i)\\right]}^{0}}{0!}\\frac{e^{-\\lambda_i(x_{i2}\\beta + \\mu_i)}{\\left[\\lambda_i(x_{i2}\\beta + \\mu_i)\\right]}^{1}}{1!}$$\n",
    "therefore we have \n",
    "$$\\frac{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})}{pr(y_{i1}=1,y_{i2}=0|u_i,x_{i1},x_{i2})+pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2})}=\n",
    "\\frac{\\lambda(x_{i1}\\beta + u_i)}{\\lambda(x_{i1}\\beta + u_i)+ \\lambda(x_{i2}\\beta + u_i)}=\\frac{e^{x'_{i1}\\beta}}{e^{x'_{i1}\\beta}+e^{x'_{i2}\\beta}}$$\n",
    "similarly \n",
    "$$pr(y_{i1}=0,y_{i2}=1|u_i,x_{i1},x_{i2}, \\sum_{t=1}^{T}{y_{it}}=1)=\\frac{e^{x'_{i2}\\beta}}{e^{x'_{i1}\\beta}+e^{x'_{i2}\\beta}}$$\n",
    "Thanks to the poisson distribution and the specification of $\\lambda$, we get rid of the $u_i$ when conditioning on $\\sum_{t=1}^{T}{y_{it}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
